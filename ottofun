#==============================================================================
#==============================================================================
# # OTTO PROGRAM
#==============================================================================
#==============================================================================

#import sys
#sys.modules[__name__].__dict__.clear()

import numpy as np
import pandas as pd
from copy import deepcopy as dcopy
#Import data
data=pd.read_csv('train.csv', sep=',')

#Pre-processing data into log(x+1)
data1=dcopy(data)
data1.iloc[:,1:-1]=np.log(data.iloc[:,1:-1]+1)

X=data.iloc[:,1:94]
Y=data.iloc[:,94]

k = np.arange(1,11)

#Creation of the matrix where I store all the test and prediction datas
ntest = 15
matrix=pd.DataFrame()

# =============================================================================
# Model 10: t-SNE - Dataset: log(X + 1)
# =============================================================================

import rpy2.robjects as robjects
from rpy2.robjects import pandas2ri
import pandas as pd
import numpy as np

pandas2ri.activate()

import rpy2.robjects.packages as rpackages

utils = rpackages.importr('utils')
utils.chooseCRANmirror(ind=1) # select the first mirror in the list
utils.install_packages('Rtsne')

df = data1
indices = np.random.permutation(len(df))

train=df.iloc[indices[:-20000],1:94]
rtrain = pandas2ri.py2ri(train)
test=df.iloc[indices[-15:],1:94]


robjects.r('''
           f <- function(train,num_rows_sample,dim) {
           
                    library(Rtsne)
                    
                    set.seed(1)
                    
                    train_sample <- train[sample(1:nrow(train), size = num_rows_sample),]
                    features     <- train_sample
                    
                    
                    tsne <- Rtsne(as.matrix(features), check_duplicates = FALSE, pca = TRUE, 
                                  perplexity=30, theta=0.5, dims=dim)
                    
                    embedding <- as.data.frame(tsne$Y)
            }
            ''')

r_f = robjects.globalenv['f']
r_Tsne=pandas2ri.ri2py((r_f(rtrain,len(train),2)))

ytrain=df.iloc[indices[:-20000],94]

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=30)
neigh.fit(train, ytrain) 

neigh.predict(test)



#==============================================================================
# MODEL 24-33: KNN - Dataset: X
#==============================================================================

from sklearn.neighbors import KNeighborsClassifier as knclass
#For loop that executes all KNN from k = 2 to k = 1024
for i in k:
    # Split data in train and test data
    # A random permutation, to split the data randomly
    np.random.seed(i)
    indices = np.random.permutation(len(Y))
    
    X_train = X.iloc[indices[:-20000],:]
    Y_train = Y[indices[:-20000]]
    X_test  = X.iloc[indices[-ntest:],:]
    Y_test  = Y[indices[-ntest:]]
    
    #KNN with k from 2 to 1024
    knn = knclass(n_neighbors=2**i)
    knn.fit(X_train, Y_train)
    Y_predict = knn.predict(X_test)
    
# Add the data to the matrix
    matrix.insert(matrix.shape[1],'Y_test_knn{}'.format(2**i),np.array(Y_test,dtype=object))       
    matrix.insert(matrix.shape[1],'Y_predict_knn{}'.format(2**i),Y_predict)

#==============================================================================
# MODEL 4: KNEIGHBORS CLASSIFIER - Dataset: Scale(Log(x+1))
#==============================================================================

from sklearn.preprocessing import scale

data2=dcopy(data1)
data2.iloc[:,1:-1]=scale(data2.iloc[:,1:-1])

X=data2.iloc[:,1:94]
Y=data2.iloc[:,94]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)
Y_predict=knn.predict(X_test)

matrix.insert(matrix.shape[1],'Y_test_knn_scale_log',np.array(Y_test,dtype=object))       
matrix.insert(matrix.shape[1],'Y_predict_knn_scale_log',Y_predict)

#==============================================================================
# MODEL 22: KNN on features X + int(X == 0)
#==============================================================================
#
# Add feature int(X==0) called 'Num_0' to data
data3=dcopy(data)
data_0 = np.count_nonzero(data==0,axis=1) 
data3.insert(data3.shape[1]-1,'Num_0',np.array(data_0,dtype=object))

X=data3.iloc[:,1:95]
Y=data3.iloc[:,95]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)
Y_predict=knn.predict(X_test)

matrix.insert(matrix.shape[1],'Y_test_knn+num0',np.array(Y_test,dtype=object))       
matrix.insert(matrix.shape[1],'Y_predict_knn+num0',Y_predict)

# =============================================================================
# Model 23: KNN on features X + int(X == 0) + log(X + 1)
# =============================================================================

# Add feature log(x+1) to data3
data4=dcopy(data3)
del i
for i in np.arange(1,data1.shape[1]-1):
    data4.insert(data4.shape[1]-1,'Feat_{}_log'.format(i),np.array(data1.iloc[:,i],dtype=object))

X=data4.iloc[:,1:-1]
Y=data4.iloc[:,-1]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)
Y_predict=knn.predict(X_test)

matrix.insert(matrix.shape[1],'Y_test_knn+num0+log',np.array(Y_test,dtype=object))       
matrix.insert(matrix.shape[1],'Y_predict_knn+num0+log',Y_predict)

#==============================================================================
# MODEL 2: LOG REGRESSION - Dataset: log(X+1))
#==============================================================================

from sklearn.linear_model import LogisticRegression as logit

#Dataset is log(x+1)

X=data1.iloc[:,1:94]
Y=data1.iloc[:,94]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#Logistic Regression
logistic = logit()
logistic.fit(X_train,Y_train)
Y_predict = logistic.predict(X_test)

# Add the data to the matrix
matrix.insert(matrix.shape[1],'Y_test_log',np.array(Y_test,dtype=object))
matrix.insert(matrix.shape[1],'Y_predict_log',Y_predict)

#==============================================================================
# MODEL 3: EXTRA TREES CLASSIFIER - Dataset: log(X+1)
#==============================================================================

from sklearn.ensemble import ExtraTreesClassifier as extreesclass

#Dataset is log(x+1)
X=data1.iloc[:,1:94]
Y=data1.iloc[:,94]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#Extra Trees Classification
extratrees = extreesclass()
extratrees.fit(X_train,Y_train)
Y_predict = extratrees.predict(X_test)

# Add the data to the matrix
matrix.insert(matrix.shape[1],'Y_test_extratrees',np.array(Y_test,dtype=object))
matrix.insert(matrix.shape[1],'Y_predict_extratrees',Y_predict)

# =============================================================================
# Model 7: Multinomial Naive Bayes - Dataset: Log(X+1)
# =============================================================================

from sklearn.naive_bayes import MultinomialNB as MNB

#Dataset is log(x+1)
X=data1.iloc[:,1:-1]
Y=data1.iloc[:,-1]

# Split data in train and test data
# A random permutation, to split the data randomly
np.random.seed(0)
indices = np.random.permutation(len(Y))

X_train = X.iloc[indices[:-20000],:]
Y_train = Y[indices[:-20000]]
X_test  = X.iloc[indices[-ntest:],:]
Y_test  = Y[indices[-ntest:]]

#Multinomial Naive Bayes Classification
multinb = MNB()
multinb.fit(X_train,Y_train)
Y_predict = multinb.predict(X_test)

# Add the data to the matrix
matrix.insert(matrix.shape[1],'Y_test_multinb',np.array(Y_test,dtype=object))
matrix.insert(matrix.shape[1],'Y_predict_multinb',Y_predict)

# =============================================================================
# Model 1: RandomForest(R) - Dataset: X
# =============================================================================

import rpy2.robjects as robjects
from rpy2.robjects import pandas2ri
import pandas as pd
import numpy as np

pandas2ri.activate()

df = data
indices = np.random.permutation(len(df))

train=df.iloc[indices[:-20000],0:94]
test=df.iloc[indices[:-20000],0:93]

#train=df.iloc[random.sample(range(1,60000), 5000),0:94]
#test=df.iloc[random.sample(range(1,60000), 100),0:93]

rtrain = pandas2ri.py2ri(train)
print(rtrain)
rtest = pandas2ri.py2ri(test)
print(rtest)

robjects.r('''
           f <- function(train) {
           
                    library(randomForest)
                    train1.rf <- randomForest(target ~ ., data = train, importance = TRUE, do.trace = 100)

            }
            ''')

r_f = robjects.globalenv['f']
rf_model=(r_f(rtrain))


robjects.r('''
           g <- function(model,test) {

                    pred <- as.data.frame(predict(model, test))

            }
            ''')

r_g = robjects.globalenv['g']
pred=pandas2ri.ri2py(r_g(rf_model,rtest))
