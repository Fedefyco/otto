#==============================================================================
#==============================================================================
# # OTTO PROGRAM
#==============================================================================
#==============================================================================

# =============================================================================
# Libraries
# =============================================================================

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale
from sklearn.neighbors import KNeighborsClassifier as knclass
from sklearn.linear_model import LogisticRegression as logit
from sklearn.naive_bayes import MultinomialNB as MNB
from sklearn.ensemble import ExtraTreesClassifier as extreesclass
from sklearn.svm import SVC
from copy import deepcopy as dcopy

import rpy2.robjects as robjects
from rpy2.robjects import pandas2ri
import rpy2.robjects.packages as rpackages

pandas2ri.activate()
# =============================================================================
# Datasets
# =============================================================================

#Import data
data_csvn_train=pd.read_csv('train.csv', sep=',')

data_csv_train=pd.read_csv('train.csv', sep=',')
data_csv_train["target"]=data_csv_train["target"].map({"Class_{}".format(k):k-1 for k in range(1,10)})
data_csv_test=pd.read_csv('test.csv', sep=',')

data_target = data_csv_train["target"]

data_raw_all=data_csv_train.iloc[:,1:-1].append(data_csv_test.iloc[:,1:94])
data_raw_train=data_raw_all.iloc[0:len(data_csv_train),:]
data_raw_test=data_raw_all.iloc[len(data_csv_train):len(data_csv_train)+len(data_csv_test),:]

data_tsne_train=pd.read_csv('train_tsne.csv', sep=',',header=None)
data_tsne_test=pd.read_csv('test_tsne.csv', sep=',',header=None)


#DATA PREPROCESSING
#Pre-processing data into Log(x+1)
#data_log=dcopy(data_raw_train)
#data_log.iloc[:,1:-1]=np.log(data_raw_train.iloc[:,1:-1]+1)

data_log_all=np.log(data_raw_all+1)
data_log_train=data_log_all.iloc[0:len(data_raw_train),:]
data_log_test=data_log_all.iloc[len(data_raw_train):len(data_raw_train)+len(data_raw_test),:]

#preprocessing data into Scale(X)
#data_scale=dcopy(data_raw_train)
#data_scale.iloc[:,1:-1]=scale(data_raw_train.iloc[:,1:-1])
data_scale_all=pd.DataFrame(scale(data_raw_all))
data_scale_train=data_scale_all.iloc[0:len(data_raw_train),:]
data_scale_test=data_scale_all.iloc[len(data_raw_train):len(data_raw_train)+len(data_raw_test),:]

#preprocessing data into Scale(Log(X + 1))
#data_scale_log=dcopy(data_log)
#data_scale_log.iloc[:,1:-1]=scale(data_log.iloc[:,1:-1])

data_scalelog_all=pd.DataFrame(scale(data_log_all))
data_scalelog_train=data_scalelog_all.iloc[0:len(data_raw_train),:]
data_scalelog_test=data_scalelog_all.iloc[len(data_raw_train):len(data_raw_train)+len(data_raw_test),:]

#preprocessing data into sqrt(X + 3/8)
#data_sqrt=dcopy(data_raw_train)
#data_sqrt.iloc[:,1:-1]=np.sqrt(data_raw_train.iloc[:,1:-1]+3/8)
data_sqrt_all=np.sqrt(data_raw_all + 3/8)
data_sqrt_train=data_sqrt_all.iloc[0:len(data_raw_train),:]
data_sqrt_test=data_sqrt_all.iloc[len(data_raw_train):len(data_raw_train)+len(data_raw_test),:]

# Add feature int(X==0) called 'Num_0' to data
#data_featnum0=dcopy(data_raw_train)
#data_0 = np.count_nonzero(data_raw_train==0,axis=1) 
#data_featnum0.insert(data_featnum0.shape[1]-1,'Num_0',np.array(data_0,dtype=object))
data_0_all = np.count_nonzero(data_raw_all==0,axis=1) 
data_0_train = np.count_nonzero(data_raw_train==0,axis=1) 
data_0_test = np.count_nonzero(data_raw_test==0,axis=1) 

# Add feature log(x+1) to data_featnum0
#data_featnum0_featlog=dcopy(data_featnum0)
#for i in range(1,data_log.shape[1]-1):
#    data_featnum0_featlog.insert(data_featnum0_featlog.shape[1]-1,'Feat_{}_log'.format(i),np.array(data_log.iloc[:,i],dtype=object))


#Creation of the matrix where I store all the test and prediction datas
testsize = 15/data_raw_train.shape[0]
trainsize = 2/3

svc_trainsize=20000

matrix_train=pd.DataFrame()
matrix_test=pd.DataFrame()

#==============================================================================
# MODEL 24-33: KNN - Dataset: X
#==============================================================================
#data = data_raw_train
#data_test = data_raw_test
##from sklearn.neighbors import KNeighborsClassifier as knclass
##For loop that executes all KNN from k = 2 to k = 1024
#for i in range(1,11):
#    # Split data in train and test data
#    X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=i)
#        
#    #KNN with k from 2 to 1024
#    knn = knclass(n_neighbors=2**i)
#    knn.fit(X_train, Y_train)
#    Ytrain_predict = knn.predict(data)
#    Ytest_predict = knn.predict(data_test)
#    
## Add the data to the matrix
#    #matrix.insert(matrix.shape[1],'Y_test_knn{}'.format(2**i),np.array(Y_test,dtype=object))       
#    matrix_train.insert(matrix_train.shape[1],'Y_predict_knn{}'.format(2**i),Ytrain_predict)
#    matrix_test.insert(matrix_test.shape[1],'Y_predict_knn{}'.format(2**i),Ytest_predict)
#
#
#matrix_train.to_csv('matrix_train_knn.csv')
#matrix_test.to_csv('matrix_test_knn.csv')

#==============================================================================
# MODEL 4: KNEIGHBORS CLASSIFIER - Dataset: Scale(Log(x+1))
#==============================================================================
data = data_scalelog_train
data_test = data_scalelog_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=11)

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)

Ytrain_predict = knn.predict(data)
Ytest_predict = knn.predict(data_test)

matrix_train.insert(matrix_train.shape[1],'Y_test_knn_scale_log',Ytrain_predict)       
matrix_test.insert(matrix_test.shape[1],'Y_predict_knn_scale_log',Ytest_predict)


pd.DataFrame(Ytrain_predict).to_csv('matrix_train_knn_scale_log.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_knn_scale_log.csv')

#==============================================================================
# MODEL 22: KNN on features X + int(X == 0)
#==============================================================================
data=data_raw_train.join(pd.DataFrame(data_0_train))
data_test=data_raw_test.join(pd.DataFrame(data_0_test))

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=12)

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)

Ytrain_predict = knn.predict(data)
Ytest_predict = knn.predict(data_test)

matrix_train.insert(matrix_train.shape[1],'Y_test_knn+num0',Ytrain_predict)       
matrix_test.insert(matrix_test.shape[1],'Y_predict_knn+num0',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_knn_x_0.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_knn_x_0.csv')

# =============================================================================
# Model 23: KNN on features X + int(X == 0) + log(X + 1)
# =============================================================================

data=pd.concat([data_raw_train, pd.DataFrame(data_0_train),data_log_train], axis=1)
data_test=pd.concat([data_raw_test, pd.DataFrame(data_0_test),data_log_test], axis=1)


X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=13)

#KNClassification
knn=knclass()
knn.fit(X_train,Y_train)

Ytrain_predict = knn.predict(data)
Ytest_predict = knn.predict(data_test)

matrix_train.insert(matrix_train.shape[1],'Y_test_knn+num0+log',Ytrain_predict)       
matrix_test.insert(matrix_test.shape[1],'Y_predict_knn+num0+log',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_knn_x_0_log.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_knn_x_0_log.csv')


#==============================================================================
# MODEL 2: LOG REGRESSION - Dataset: log(X+1))
#==============================================================================
data = data_scalelog_train
data_test = data_scalelog_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=14)

#Logistic Regression
logistic = logit()
logistic.fit(X_train,Y_train)

Ytrain_predict = logistic.predict(data)
Ytest_predict = logistic.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_logit',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_logit',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_logit_log.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_logit_log.csv')

#==============================================================================
# MODEL 3: EXTRA TREES CLASSIFIER - Dataset: log(X+1)
#==============================================================================
data = data_log_train
data_test = data_log_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=15)

#Extra Trees Classification
extratrees = extreesclass()
extratrees.fit(X_train,Y_train)

Ytrain_predict = extratrees.predict(data)
Ytest_predict = extratrees.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_extratrees',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_extratrees',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_extratrees_log.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_extratrees_log.csv')

# =============================================================================
# Model 7: Multinomial Naive Bayes - Dataset: Log(X+1)
# =============================================================================
data = data_log_train
data_test = data_log_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=16)

#Multinomial Naive Bayes Classification
multinb = MNB()
multinb.fit(X_train,Y_train)

Ytrain_predict = multinb.predict(data)
Ytest_predict = multinb.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_multinb',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_multinb',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_mnb_log.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_mnb_log.csv')


# =============================================================================
# Model 11: SVM(from scikit) - Dataset: scale(X)
# =============================================================================
data = data_scale_train
data_test = data_scale_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=svc_trainsize, random_state=17)

clf = SVC(gamma=2, C=1)
clf.fit(X_train, Y_train) 

Ytrain_predict = clf.predict(data)
Ytest_predict = clf.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_svc_scalex',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_svc_scalex',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_svc_scalex.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_svc_scalex.csv')

# =============================================================================
# Model 12: SVM(from scikit) - Dataset: X + T-SNE
# =============================================================================

data = pd.concat([pd.DataFrame(scale(data_raw_train)), pd.DataFrame(scale(data_tsne_train))], axis=1)
data_test = pd.concat([pd.DataFrame(scale(data_raw_test)), pd.DataFrame(scale(data_tsne_test))], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=svc_trainsize, random_state=18)


clf = SVC(gamma=2, C=1)
clf.fit(X_train, Y_train) 

Ytrain_predict = clf.predict(data)
Ytest_predict = clf.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_svc_x_tsne',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_svc_x_tsne',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_svc_x_tsne.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_svc_x_tsne.csv')

# =============================================================================
# Model 13: SVM(from scikit) - Dataset: Log(X + T-SNE)
# =============================================================================

data = pd.concat([pd.DataFrame(scale(np.log(1+data_raw_train))), pd.DataFrame(scale(np.log(1+np.abs(data_tsne_train))))], axis=1)
data_test = pd.concat([pd.DataFrame(scale(np.log(1+data_raw_test))), pd.DataFrame(scale(np.log(1+np.abs(data_tsne_test))))], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=svc_trainsize, random_state=19)

clf = SVC(gamma=2, C=1)
clf.fit(X_train, Y_train) 

Ytrain_predict = clf.predict(data)
Ytest_predict = clf.predict(data_test)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_svc_log_x_tsne',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_svc_log_x_tsne',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_svc_log_x_tsne.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_svc_log_x_tsne.csv')

# =============================================================================
# Model 1: RandomForest(R) - Dataset: X
# =============================================================================
data = data_raw_train
data_test = data_raw_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_target, test_size=testsize, train_size=trainsize, random_state=20)

dtest=pd.concat([X_train,Y_train], axis=1)
rtrain = pandas2ri.py2ri(dtest)
print(rtrain)
rtest = pandas2ri.py2ri(data_test)
print(rtest)

robjects.r('''
           f <- function(train) {
           
                    library(randomForest)
                    train1.rf <- randomForest(target ~ ., data=train, importance = TRUE, do.trace = 100)

            }
            ''')

utils = rpackages.importr('utils')
utils.chooseCRANmirror(ind=1) # select the first mirror in the list
#utils.install_packages('randomForest')

r_f = robjects.globalenv['f']
rf_model=(r_f(rtrain))


robjects.r('''
           g <- function(model,test) {

                    pred <- as.data.frame(predict(model, test))

            }
            ''')

r_g = robjects.globalenv['g']


Ytrain_predict = pandas2ri.ri2py(r_g(rf_model,pandas2ri.py2ri(data)))
Ytest_predict = pandas2ri.ri2py(r_g(rf_model,pandas2ri.py2ri(data_test)))

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_r_rf',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_r_rf',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_r_rf_x.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_r_rf_x.csv')

# =============================================================================
# Model 14-18: Xgboost(from R) - Dataset: X
# =============================================================================

#df = data
#indices = np.random.permutation(len(df))
#
#datatrain=df.iloc[indices[:5000],:]
#rdatatrain = pandas2ri.py2ri(dtest)
#
#datatest=df.iloc[indices[-15:],:]
#rdatatest = pandas2ri.py2ri(data_test)

data = data_raw_train
data_test = data_raw_test

X_train, X_test, Y_train, Y_test = train_test_split(data, data_csvn_train.iloc[:,94], test_size=testsize, train_size=trainsize, random_state=21)

dtest=pd.concat([X_train,Y_train], axis=1)
r_xgb1_datatrain = pandas2ri.py2ri(dtest)
print(r_xgb1_datatrain)
r_xgb1_datatest = pandas2ri.py2ri(data_test)
print(r_xgb1_datatest)

######################################
# import R's utility package
utils = rpackages.importr('utils')
utils.chooseCRANmirror(ind=1) # select the first mirror in the list
#utils.install_packages('xgboost')
#utils.install_packages('methods')
#utils.install_packages('data.table')
#utils.install_packages('magrittr')
#utils.install_packages('Ckmeans.1d.dp')
######################################



robjects.r('''
           f <- function(rdata,nround) {
            
            library(xgboost)
            library(methods)
            #install.packages("data.table", dependencies=TRUE)
            library(data.table)
            library(magrittr)
            library(Ckmeans.1d.dp)

            train<-data.table(rdata)
            
            # Delete ID column in training dataset
            #train[, id := NULL]
            
            # Save the name of the last column
            nameLastCol <- names(train)[ncol(train)]
            
            # Convert from classes to numbers
            y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}


            
            train[, nameLastCol:=NULL, with = F]
            
            trainMatrix <- train[,lapply(.SD,as.numeric)] %>% as.matrix
            
            numberOfClasses <- max(y) + 1
            param <- list("objective" = "multi:softprob",
                          "eval_metric" = "mlogloss",
                          "num_class" = numberOfClasses)
            
            
            bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)

            
            }
            ''')


r_f = robjects.globalenv['f']
rf_xgboostmodel=(r_f(r_xgb1_datatrain,50))
pandas2ri.ri2py(rf_xgboostmodel)

robjects.r('''
           g <- function(model,rdatatest,numberOfClasses) {
                    
                    library(xgboost)
                    library(methods)
                    library(data.table)
                    library(magrittr)
                    library(Ckmeans.1d.dp)

           
                    test<-data.table(rdatatest)
                    
                    # Delete ID column in training dataset
                    #test[, id := NULL]
                    
                    # Save the name of the last column
                    nameLastCol <- names(test)[ncol(test)]
                    
                    # Convert from classes to numbers
        
                    #test[, nameLastCol:=NULL, with = F]
                    
                    testMatrix <- test[,lapply(.SD,as.numeric)] %>% as.matrix


                    pred <- predict(model, testMatrix)
                    pred_ok<- matrix(pred,length(pred)/numberOfClasses, numberOfClasses,byrow = TRUE)

            }
            ''')

r_g = robjects.globalenv['g']

xgboostpredtrain=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data),9)))
xgboostpredtrain.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytrain_predict=xgboostpredtrain.idxmax(axis=1)

xgboostpredtest=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data_test),9)))
xgboostpredtest.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytest_predict=xgboostpredtest.idxmax(axis=1)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_r_xgb1',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_r_xgb1',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_r_xgb1_x.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_r_xgb1_x.csv')

# =============================================================================
# Model 14-18: Xgboost(from R) - Dataset: X + Tsne
# =============================================================================

#df = data
#indices = np.random.permutation(len(df))
#
#datatrain=df.iloc[indices[:5000],:]
#rdatatrain = pandas2ri.py2ri(dtest)
#
#datatest=df.iloc[indices[-15:],:]
#rdatatest = pandas2ri.py2ri(data_test)

data = pd.concat([ pd.DataFrame(scale(data_raw_train)), pd.DataFrame(scale(data_tsne_train))], axis=1)
data_test = pd.concat([ pd.DataFrame(scale(data_raw_test)), pd.DataFrame(scale(data_tsne_test))], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(data, data_csvn_train.iloc[:,94], test_size=testsize, train_size=trainsize, random_state=22)

dtest=pd.concat([X_train,Y_train], axis=1)
r_xgb1_datatrain = pandas2ri.py2ri(dtest)
print(r_xgb1_datatrain)
r_xgb1_datatest = pandas2ri.py2ri(data_test)
print(r_xgb1_datatest)

######################################
# import R's utility package
utils = rpackages.importr('utils')
utils.chooseCRANmirror(ind=1) # select the first mirror in the list
#utils.install_packages('xgboost')
#utils.install_packages('methods')
#utils.install_packages('data.table')
#utils.install_packages('magrittr')
#utils.install_packages('Ckmeans.1d.dp')
######################################



robjects.r('''
           f <- function(rdata,nround) {
            
            library(xgboost)
            library(methods)
            #install.packages("data.table", dependencies=TRUE)
            library(data.table)
            library(magrittr)
            library(Ckmeans.1d.dp)

            train<-data.table(rdata)
            
            # Delete ID column in training dataset
            #train[, id := NULL]
            
            # Save the name of the last column
            nameLastCol <- names(train)[ncol(train)]
            
            # Convert from classes to numbers
            y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}


            
            train[, nameLastCol:=NULL, with = F]
            
            trainMatrix <- train[,lapply(.SD,as.numeric)] %>% as.matrix
            
            numberOfClasses <- max(y) + 1
            param <- list("objective" = "multi:softprob",
                          "eval_metric" = "mlogloss",
                          "num_class" = numberOfClasses)
            
            
            bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)

            
            }
            ''')


r_f = robjects.globalenv['f']
rf_xgboostmodel=(r_f(r_xgb1_datatrain,50))
pandas2ri.ri2py(rf_xgboostmodel)

robjects.r('''
           g <- function(model,rdatatest,numberOfClasses) {
                    
                    library(xgboost)
                    library(methods)
                    library(data.table)
                    library(magrittr)
                    library(Ckmeans.1d.dp)

           
                    test<-data.table(rdatatest)
                    
                    # Delete ID column in training dataset
                    #test[, id := NULL]
                    
                    # Save the name of the last column
                    nameLastCol <- names(test)[ncol(test)]
                    
                    # Convert from classes to numbers
        
                    #test[, nameLastCol:=NULL, with = F]
                    
                    testMatrix <- test[,lapply(.SD,as.numeric)] %>% as.matrix


                    pred <- predict(model, testMatrix)
                    pred_ok<- matrix(pred,length(pred)/numberOfClasses, numberOfClasses,byrow = TRUE)

            }
            ''')

r_g = robjects.globalenv['g']

xgboostpredtrain=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data),9)))
xgboostpredtrain.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytrain_predict=xgboostpredtrain.idxmax(axis=1)

xgboostpredtest=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data_test),9)))
xgboostpredtest.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytest_predict=xgboostpredtest.idxmax(axis=1)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_r_xgb2_x_tsne',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_r_xgb2_x_tsne',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_r_xgb2_x_tsne.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_r_xgb2_x_tsne.csv')

# =============================================================================
# Model 14-18: Xgboost(from R) - Dataset: log(X + Tsne)
# =============================================================================

#df = data
#indices = np.random.permutation(len(df))
#
#datatrain=df.iloc[indices[:5000],:]
#rdatatrain = pandas2ri.py2ri(dtest)
#
#datatest=df.iloc[indices[-15:],:]
#rdatatest = pandas2ri.py2ri(data_test)

data = pd.concat([pd.DataFrame(scale(np.log(1+data_raw_train))), pd.DataFrame(scale(np.log(1+np.abs(data_tsne_train))))], axis=1)
data_test = pd.concat([pd.DataFrame(scale(np.log(1+data_raw_test))), pd.DataFrame(scale(np.log(1+np.abs(data_tsne_test))))], axis=1)

X_train, X_test, Y_train, Y_test = train_test_split(data, data_csvn_train.iloc[:,94], test_size=testsize, train_size=trainsize, random_state=23)

dtest=pd.concat([X_train,Y_train], axis=1)
r_xgb1_datatrain = pandas2ri.py2ri(dtest)
print(r_xgb1_datatrain)
r_xgb1_datatest = pandas2ri.py2ri(data_test)
print(r_xgb1_datatest)

######################################
# import R's utility package
utils = rpackages.importr('utils')
utils.chooseCRANmirror(ind=1) # select the first mirror in the list
#utils.install_packages('xgboost')
#utils.install_packages('methods')
#utils.install_packages('data.table')
#utils.install_packages('magrittr')
#utils.install_packages('Ckmeans.1d.dp')
######################################



robjects.r('''
           f <- function(rdata,nround) {
            
            library(xgboost)
            library(methods)
            #install.packages("data.table", dependencies=TRUE)
            library(data.table)
            library(magrittr)
            library(Ckmeans.1d.dp)

            train<-data.table(rdata)
            
            # Delete ID column in training dataset
            #train[, id := NULL]
            
            # Save the name of the last column
            nameLastCol <- names(train)[ncol(train)]
            
            # Convert from classes to numbers
            y <- train[, nameLastCol, with = F][[1]] %>% gsub('Class_','',.) %>% {as.integer(.) -1}


            
            train[, nameLastCol:=NULL, with = F]
            
            trainMatrix <- train[,lapply(.SD,as.numeric)] %>% as.matrix
            
            numberOfClasses <- max(y) + 1
            param <- list("objective" = "multi:softprob",
                          "eval_metric" = "mlogloss",
                          "num_class" = numberOfClasses)
            
            
            bst = xgboost(param=param, data = trainMatrix, label = y, nrounds=nround)

            
            }
            ''')


r_f = robjects.globalenv['f']
rf_xgboostmodel=(r_f(r_xgb1_datatrain,50))
pandas2ri.ri2py(rf_xgboostmodel)

robjects.r('''
           g <- function(model,rdatatest,numberOfClasses) {
                    
                    library(xgboost)
                    library(methods)
                    library(data.table)
                    library(magrittr)
                    library(Ckmeans.1d.dp)

           
                    test<-data.table(rdatatest)
                    
                    # Delete ID column in training dataset
                    #test[, id := NULL]
                    
                    # Save the name of the last column
                    nameLastCol <- names(test)[ncol(test)]
                    
                    # Convert from classes to numbers
        
                    #test[, nameLastCol:=NULL, with = F]
                    
                    testMatrix <- test[,lapply(.SD,as.numeric)] %>% as.matrix


                    pred <- predict(model, testMatrix)
                    pred_ok<- matrix(pred,length(pred)/numberOfClasses, numberOfClasses,byrow = TRUE)

            }
            ''')

r_g = robjects.globalenv['g']

xgboostpredtrain=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data),9)))
xgboostpredtrain.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytrain_predict=xgboostpredtrain.idxmax(axis=1)

xgboostpredtest=pd.DataFrame(pandas2ri.ri2py(r_g(rf_xgboostmodel,pandas2ri.py2ri(data_test),9)))
xgboostpredtest.columns = ['Class_1','Class_2','Class_3','Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']
Ytest_predict=xgboostpredtest.idxmax(axis=1)

# Add the data to the matrix
matrix_train.insert(matrix_train.shape[1],'Y_test_r_xgb3_log_x_tsne',Ytrain_predict)
matrix_test.insert(matrix_test.shape[1],'Y_predict_r_xgb3_log_x_tsne',Ytest_predict)

pd.DataFrame(Ytrain_predict).to_csv('matrix_train_r_xgb3_log_x_tsne.csv')
pd.DataFrame(Ytest_predict).to_csv('matrix_test_r_xgb3_log_x_tsne.csv')
